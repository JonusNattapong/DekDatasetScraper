#!/usr/bin/env python3
"""
Ratchakitcha.soc.go.th Dataset Scraper - CRAWL4AI FIXED VERSION
‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå PDF ‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏£‡∏≤‡∏ä‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏ô‡∏∏‡πÄ‡∏ö‡∏Å‡∏©‡∏≤
‡πÉ‡∏ä‡πâ crawl4ai ‡πÅ‡∏ï‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Windows + Anti-Block
"""

import asyncio
import json
import pandas as pd
from urllib.parse import urljoin, urlparse
import logging
import re
from typing import List, Dict, Optional, Set
import aiofiles
import aiohttp
import os
from datetime import datetime
import hashlib
import time
from pathlib import Path
import gc
from mistralai import Mistral
try:
    from openai import OpenAI  # DeepSeek ‡πÉ‡∏ä‡πâ OpenAI format
    DEEPSEEK_AVAILABLE = True
except ImportError:
    DEEPSEEK_AVAILABLE = False
import sys
import random
import platform

# PyThaiNLP for text correction - Optimized
try:
    # Import ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ callable
    import pythainlp
    from pythainlp.correct import correct
    from pythainlp import word_tokenize
    PYTHAINLP_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("‚úÖ PyThaiNLP imported successfully (optimized)")
except ImportError as e:
    PYTHAINLP_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"‚ö†Ô∏è PyThaiNLP not available: {e}")

# Cache ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PyThaiNLP ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
_pythainlp_cache = {}

# env
from dotenv import load_dotenv
load_dotenv()

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Event Loop ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows Python 3.13
def fix_windows_event_loop():
    """‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ NotImplementedError ‡πÉ‡∏ô Windows"""
    if platform.system() == 'Windows':
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Python 3.13 ‡πÉ‡∏ô Windows
        if sys.version_info >= (3, 8):
            try:
                # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ ProactorEventLoopPolicy ‡∏Å‡πà‡∏≠‡∏ô (‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö subprocess)
                asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
                logger.info("‚úÖ ‡πÉ‡∏ä‡πâ WindowsProactorEventLoopPolicy")
            except Exception as e:
                try:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏ä‡πâ SelectorEventLoopPolicy
                    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
                    logger.info("‚úÖ ‡πÉ‡∏ä‡πâ WindowsSelectorEventLoopPolicy")
                except Exception as e2:
                    logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ event loop policy: {e2}")

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Windows event loop ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
fix_windows_event_loop()

# Import crawl4ai ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç event loop
try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
    CRAWL4AI_AVAILABLE = True
    logger.info("‚úÖ crawl4ai imported successfully")
except ImportError as e:
    logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ import crawl4ai: {e}")
    logger.error("üí° ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢: pip install crawl4ai")
    CRAWL4AI_AVAILABLE = False
except Exception as e:
    logger.error(f"‚ùå crawl4ai error: {e}")
    CRAWL4AI_AVAILABLE = False

class CacheManager:
    """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Cache ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR Results"""
    
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.memory_cache = {}
        
    def _get_cache_key(self, url: str) -> str:
        return hashlib.md5(url.encode()).hexdigest()
    
    async def get(self, url: str) -> Optional[str]:
        cache_key = self._get_cache_key(url)
        
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]
        
        cache_file = self.cache_dir / f"{cache_key}.txt"
        if cache_file.exists():
            try:
                async with aiofiles.open(cache_file, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    self.memory_cache[cache_key] = content
                    return content
            except Exception as e:
                logger.warning(f"Error reading cache: {e}")
        
        return None
    
    async def set(self, url: str, content: str):
        cache_key = self._get_cache_key(url)
        self.memory_cache[cache_key] = content
        
        cache_file = self.cache_dir / f"{cache_key}.txt"
        try:
            async with aiofiles.open(cache_file, 'w', encoding='utf-8') as f:
                await f.write(content)
        except Exception as e:
            logger.warning(f"Error writing cache: {e}")

class ProgressTracker:
    """‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤"""
    
    def __init__(self, total: int):
        self.total = total
        self.completed = 0
        self.start_time = time.time()
        self.failed = 0
    
    def update(self, success: bool = True):
        self.completed += 1
        if not success:
            self.failed += 1
        
        if self.completed % 5 == 0 or self.completed == self.total:
            elapsed = time.time() - self.start_time
            rate = self.completed / elapsed if elapsed > 0 else 0
            eta = (self.total - self.completed) / rate if rate > 0 else 0
            
            logger.info(f"üöÄ Progress: {self.completed}/{self.total} ({self.completed/self.total*100:.1f}%) "
                       f"Rate: {rate:.1f}/s ETA: {eta/60:.1f}m Failed: {self.failed}")

class MistralOCRWithDeepSeekCorrection:
    """‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ä‡πâ Mistral OCR + DeepSeek ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î"""
    
    def __init__(self, use_deepseek_correction=False):
        self.use_deepseek_correction = use_deepseek_correction
        self.cache = CacheManager()
        self.embedding_cache = {}  # Cache ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö embeddings
        
        # Mistral API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö)
        mistral_key = os.getenv("MISTRAL_API_KEY")
        if not mistral_key:
            raise ValueError("MISTRAL_API_KEY is required for OCR")
        
        self.mistral_client = Mistral(api_key=mistral_key)
        self.embedding_model = "mistral-embed"
        self.ocr_model = "mistral-ocr-latest"
        
        # DeepSeek API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        self.deepseek_client = None
        if use_deepseek_correction and DEEPSEEK_AVAILABLE:
            deepseek_key = os.getenv("DEEPSEEK_API_KEY")
            if deepseek_key:
                self.deepseek_client = OpenAI(
                    api_key=deepseek_key,
                    base_url="https://api.deepseek.com/v1"
                )
                logger.info("‚úÖ Mistral OCR + DeepSeek Text Correction ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!")
            else:
                logger.warning("‚ö†Ô∏è DEEPSEEK_API_KEY ‡πÑ‡∏°‡πà‡∏û‡∏ö ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Pattern-based correction")
        else:
            logger.info("‚úÖ Mistral OCR + Pattern-based correction ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    
    async def extract_text_from_pdf_url_cached(self, pdf_url: str) -> str:
        cached_result = await self.cache.get(pdf_url)
        if cached_result:
            logger.debug(f"üìö Cache hit: {pdf_url}")
            return cached_result
        
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ url ‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢ .pdf ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å OCR
            if not pdf_url.lower().endswith(".pdf"):
                logger.error(f"‚ùå Invalid PDF URL (not .pdf): {pdf_url}")
                return ""
            # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î PDF ‡∏°‡∏≤‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏¢‡∏±‡∏á API
            try:
                import tempfile
                import aiohttp
                with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmpfile:
                    tmp_path = tmpfile.name
                async with aiohttp.ClientSession() as session:
                    async with session.get(pdf_url) as resp:
                        if resp.status == 200:
                            with open(tmp_path, "wb") as f:
                                f.write(await resp.read())
                        else:
                            logger.error(f"‚ùå Download failed: {pdf_url} (HTTP {resp.status})")
                            return f"Download failed: HTTP {resp.status}"
                with open(tmp_path, "rb") as pdf_file:
                    uploaded_file = self.mistral_client.files.upload(
                        file={
                            "file_name": tmp_path.split("/")[-1],
                            "content": pdf_file,
                        },
                        purpose="ocr"
                    )
                # get signed url
                signed_url = self.mistral_client.files.get_signed_url(file_id=uploaded_file.id)
                ocr_response = self.mistral_client.ocr.process(
                    model="mistral-ocr-latest",
                    document={
                        "type": "document_url",
                        "document_url": signed_url.url
                    },
                    include_image_base64=False
                )
            finally:
                import os
                if os.path.exists(tmp_path):
                    os.remove(tmp_path)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å OCR ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
            if not ocr_response:
                return "OCR Error: ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å API"
            
            # Debug OCR response structure
            logger.debug(f"OCR Response type: {type(ocr_response)}")
            logger.debug(f"OCR Response dir: {dir(ocr_response)}")
            
            result = ""
            
            # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å Mistral OCR response
            try:
                # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å attribute ‡∏ï‡πà‡∏≤‡∏á‡πÜ
                if hasattr(ocr_response, "pages") and ocr_response.pages:
                    # Mistral OCR ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô pages
                    pages_text = []
                    for page in ocr_response.pages:
                        if hasattr(page, "markdown") and page.markdown:
                            # Mistral OCR ‡πÉ‡∏ä‡πâ markdown format
                            pages_text.append(page.markdown)
                        elif hasattr(page, "text") and page.text:
                            pages_text.append(page.text)
                        elif hasattr(page, "content") and page.content:
                            pages_text.append(page.content)
                    if pages_text:
                        result = "\n".join(pages_text)
                        
                elif hasattr(ocr_response, "text") and ocr_response.text:
                    result = ocr_response.text
                elif hasattr(ocr_response, "content") and ocr_response.content:
                    result = ocr_response.content
                elif hasattr(ocr_response, "result") and ocr_response.result:
                    result = ocr_response.result
                elif hasattr(ocr_response, "data") and ocr_response.data:
                    result = ocr_response.data
                elif hasattr(ocr_response, "extracted_text"):
                    result = ocr_response.extracted_text
                elif isinstance(ocr_response, dict):
                    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô dict
                    result = (
                        ocr_response.get("text")
                        or ocr_response.get("content") 
                        or ocr_response.get("result")
                        or ocr_response.get("data")
                        or ocr_response.get("extracted_text")
                        or ""
                    )
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏•‡∏≠‡∏á‡∏î‡∏π‡πÉ‡∏ô pages
                    if not result and "pages" in ocr_response:
                        pages = ocr_response["pages"]
                        if isinstance(pages, list):
                            pages_text = []
                            for page in pages:
                                if isinstance(page, dict):
                                    page_text = page.get("text") or page.get("content") or ""
                                    if page_text:
                                        pages_text.append(page_text)
                            result = "\n".join(pages_text)
                
                # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏î‡∏π raw response
                if not result:
                    logger.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ OCR Response: {str(ocr_response)[:500]}")
                    return f"OCR Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô response (Type: {type(ocr_response).__name__})"
                    
            except Exception as e:
                logger.error(f"Error parsing OCR response: {e}")
                return f"OCR Error: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô response ({str(e)})"
            
            if result:
                await self.cache.set(pdf_url, result)
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå OCR Error for {pdf_url}: {str(e)}")
            return ""
    
    async def create_text_embeddings(self, text_chunks: List[str]) -> List[List[float]]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î"""
        try:
            if not text_chunks:
                return []
            
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk
            processed_chunks = []
            for chunk in text_chunks:
                if len(chunk) > 1000:  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 1000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
                    chunk = chunk[:1000]
                processed_chunks.append(chunk)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏î‡πâ‡∏ß‡∏¢ Mistral
            embeddings_response = self.mistral_client.embeddings.create(
                model=self.embedding_model,
                inputs=processed_chunks,
            )
            
            # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ embeddings data
            embeddings = []
            for data in embeddings_response.data:
                embeddings.append(data.embedding)
            
            logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(embeddings)} chunks")
            return embeddings
            
        except Exception as e:
            logger.error(f"‚ùå Embeddings Error: {str(e)}")
            return []
    
    async def correct_text_with_deepseek(self, error_text: str) -> str:
        """‡πÉ‡∏ä‡πâ DeepSeek ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î"""
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö cache
            error_hash = hashlib.md5(error_text.encode()).hexdigest()[:8]
            if error_hash in self.embedding_cache:
                return self.embedding_cache[error_hash]
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ DeepSeek client ‡πÉ‡∏´‡πâ return ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
            if not self.deepseek_client:
                return error_text
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î
            prompt = f"""‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏à‡∏≤‡∏Å OCR:

‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {error_text}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:
1. ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà OCR ‡∏≠‡πà‡∏≤‡∏ô‡∏ú‡∏¥‡∏î ‡πÄ‡∏ä‡πà‡∏ô "‡∏∞" ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô "‡πñ", "‡∏ï‡∏°" ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô "‡∏Å‡∏°"
2. ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î ‡πÄ‡∏ä‡πà‡∏ô "‡πí‡πï‡∏∞‡πò" ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô "‡πí‡πï‡πñ‡πò"
3. ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£‡πÑ‡∏ó‡∏¢
4. ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ú‡∏¥‡∏î

‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:"""

            response = await asyncio.get_event_loop().run_in_executor(
                None, 
                lambda: self.deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏à‡∏≤‡∏Å OCR ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=1000,
                    temperature=0.1
                )
            )
            
            corrected_text = response.choices[0].message.content.strip()
            
            # Cache ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            self.embedding_cache[error_hash] = corrected_text
            
            logger.info(f"ü§ñ DeepSeek ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: '{error_text[:30]}...' ‚Üí '{corrected_text[:30]}...'")
            return corrected_text
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è DeepSeek correction failed: {e}")
            return error_text

class RatchakitchaScraperCrawl4AI:
    """Scraper ‡πÉ‡∏ä‡πâ crawl4ai ‡πÅ‡∏ï‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Windows"""
    
    def __init__(self):
        self.base_url = "https://ratchakitcha.soc.go.th"
        self.documents_pattern = r'(?:href|src)=["\']([^"\']*documents/[^"\']*\.pdf)["\']'
        self.processed_urls: Set[str] = set()
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô OCR - Mistral OCR + DeepSeek Text Correction
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ DeepSeek ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            use_deepseek_correction = DEEPSEEK_AVAILABLE and os.getenv("DEEPSEEK_API_KEY")
            self.ocr = MistralOCRWithDeepSeekCorrection(use_deepseek_correction=use_deepseek_correction)
        except ValueError as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô OCR: {e}")
            self.ocr = None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ crawl4ai ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        self.use_crawl4ai = CRAWL4AI_AVAILABLE
        
        if self.use_crawl4ai:
            # Browser config - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows
            self.browser_config = BrowserConfig(
                headless=True,
                verbose=False,
                browser_type="chromium",
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                extra_args=[
                    "--no-sandbox",
                    "--disable-dev-shm-usage", 
                    "--disable-gpu",
                    "--disable-web-security",
                    "--disable-features=VizDisplayCompositor",
                    "--disable-background-timer-throttling",
                    "--disable-renderer-backgrounding",
                    "--disable-backgrounding-occluded-windows",
                    "--disable-ipc-flooding-protection",
                    "--no-first-run",
                    "--no-default-browser-check",
                    "--disable-default-apps",
                    "--disable-extensions",
                    "--disable-plugins",
                    "--disable-sync",
                    "--disable-translate",
                    "--hide-scrollbars",
                    "--mute-audio",
                    "--no-zygote",  # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows
                    "--single-process"  # ‡∏•‡∏î process overhead
                ]
            )
            
            # Crawler config - ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£
            self.run_config = CrawlerRunConfig(
                cache_mode=CacheMode.ENABLED,
                word_count_threshold=5,
                wait_for_images=False,
                delay_before_return_html=2.0,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£
                page_timeout=20000,  # 20 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
                remove_overlay_elements=True,
                simulate_user=True,  # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á
                override_navigator=True  # ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö bot
            )
        
        # Fallback headers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö aiohttp
        self.fallback_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'th-TH,th;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }

    async def discover_pdf_links_crawl4ai(self, start_url: str = None, max_pages: int = 25) -> List[Dict]:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå PDF ‡∏î‡πâ‡∏ß‡∏¢ crawl4ai (Fixed for Windows)"""
        if not start_url:
            start_url = self.base_url
            
        all_pdf_links = []
        logger.info(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå PDF ‡∏î‡πâ‡∏ß‡∏¢ crawl4ai (Windows Fixed): {start_url}")
        
        if not self.use_crawl4ai:
            logger.error("‚ùå crawl4ai ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ aiohttp")
            return await self.discover_pdf_links_fallback(start_url, max_pages)
        
        try:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á new event loop ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö crawl4ai
            if platform.system() == 'Windows':
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows ‡πÉ‡∏ä‡πâ ProactorEventLoop
                try:
                    loop = asyncio.ProactorEventLoop()
                    asyncio.set_event_loop(loop)
                except:
                    # fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                    pass
            
            async with AsyncWebCrawler(config=self.browser_config) as crawler:
                logger.info("üåê ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô crawl4ai browser...")
                
                # ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏î‡πâ‡∏ß‡∏¢ retry
                result = None
                for attempt in range(3):
                    try:
                        logger.info(f"üìÑ ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt + 1})...")
                        result = await crawler.arun(url=start_url, config=self.run_config)
                        if result.success:
                            break
                        else:
                            logger.warning(f"‚ö†Ô∏è ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt + 1} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {result.error_message}")
                            if attempt < 2:
                                await asyncio.sleep(random.uniform(2, 5))
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt + 1} ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
                        if attempt < 2:
                            await asyncio.sleep(random.uniform(2, 5))
                
                if result and result.success:
                    logger.info(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(result.html)} characters)")
                    
                    # ‡∏´‡∏≤‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå PDF ‡πÅ‡∏•‡∏∞ navigation links ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
                    pdf_task = self._extract_pdf_links_from_html(result.html, start_url)
                    nav_task = self._find_navigation_links(result.html, start_url)
                    
                    pdf_links, page_links = await asyncio.gather(pdf_task, nav_task)
                    all_pdf_links.extend(pdf_links)
                    
                    logger.info(f"üìÑ ‡∏û‡∏ö PDF {len(pdf_links)} ‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å")
                    
                    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤
                    page_links = page_links[:max_pages]
                    logger.info(f"üîó ‡∏à‡∏∞‡∏™‡πÅ‡∏Å‡∏ô {len(page_links)} ‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°")
                    
                    # ‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î concurrent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows
                    if page_links:
                        semaphore = asyncio.Semaphore(8)  # ‡∏•‡∏î‡∏•‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows
                        
                        async def scan_page(page_url):
                            async with semaphore:
                                try:
                                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° delay ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°
                                    await asyncio.sleep(random.uniform(1, 3))
                                    
                                    page_result = await crawler.arun(url=page_url, config=self.run_config)
                                    if page_result.success:
                                        return await self._extract_pdf_links_from_html(page_result.html, page_url)
                                    else:
                                        logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á {page_url}: {page_result.error_message}")
                                except Exception as e:
                                    logger.warning(f"Error scanning {page_url}: {e}")
                                return []
                        
                        # ‡∏£‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
                        tasks = [scan_page(url) for url in page_links]
                        results = await asyncio.gather(*tasks, return_exceptions=True)
                        
                        for i, result in enumerate(results):
                            if isinstance(result, list):
                                all_pdf_links.extend(result)
                                if result:
                                    logger.info(f"   ‚úÖ ‡∏´‡∏ô‡πâ‡∏≤ {i+1}: ‡∏û‡∏ö {len(result)} PDF")
                            elif isinstance(result, Exception):
                                logger.warning(f"   ‚ùå ‡∏´‡∏ô‡πâ‡∏≤ {i+1}: {result}")
                else:
                    logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏î‡πâ‡∏ß‡∏¢ crawl4ai")
                    return await self.discover_pdf_links_fallback(start_url, max_pages)
                
        except Exception as e:
            logger.error(f"‚ùå crawl4ai Error: {e}")
            logger.info("üîÑ ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ aiohttp fallback...")
            return await self.discover_pdf_links_fallback(start_url, max_pages)
        
        # ‡∏•‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
        unique_pdf_links = self._remove_duplicate_links_fast(all_pdf_links)
        logger.info(f"üéâ ‡∏û‡∏ö‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå PDF ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(unique_pdf_links)} ‡πÑ‡∏ü‡∏•‡πå")
        
        return unique_pdf_links

    async def discover_pdf_links_fallback(self, start_url: str, max_pages: int) -> List[Dict]:
        """Fallback ‡πÉ‡∏ä‡πâ aiohttp ‡πÄ‡∏°‡∏∑‡πà‡∏≠ crawl4ai ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ"""
        logger.info("üîÑ ‡πÉ‡∏ä‡πâ aiohttp fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ PDF")
        
        all_pdf_links = []
        timeout = aiohttp.ClientTimeout(total=30)
        
        try:
            async with aiohttp.ClientSession(timeout=timeout, headers=self.fallback_headers) as session:
                # ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å
                async with session.get(start_url) as response:
                    if response.status == 200:
                        content = await response.text()
                        pdf_links = await self._extract_pdf_links_from_html(content, start_url)
                        all_pdf_links.extend(pdf_links)
                        logger.info(f"üìÑ aiohttp fallback: ‡∏û‡∏ö {len(pdf_links)} PDF")
                    else:
                        logger.warning(f"aiohttp fallback HTTP {response.status}")
        except Exception as e:
            logger.error(f"‚ùå aiohttp fallback error: {e}")
        
        return self._remove_duplicate_links_fast(all_pdf_links)

    async def _extract_pdf_links_from_html(self, html: str, base_url: str) -> List[Dict]:
        """‡πÅ‡∏¢‡∏Å‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå PDF ‡∏à‡∏≤‡∏Å HTML - Optimized"""
        pdf_links = []
        
        # ‡∏£‡∏ß‡∏° patterns ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        all_patterns = [
            self.documents_pattern,
            r'href=["\']([^"\']*\.pdf)["\']',
            r'documents/(\d+)\.pdf',
            r'/documents/([^"\']*\.pdf)',
            r'href=["\']([^"\']*\.PDF)["\']',  # uppercase
            r'src=["\']([^"\']*\.pdf)["\']'
        ]
        
        for pattern in all_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            for match in matches:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á URL ‡πÄ‡∏ï‡πá‡∏°
                if match.startswith('http'):
                    full_url = match
                else:
                    full_url = urljoin(base_url, match)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ã‡πâ‡∏≥
                if full_url not in self.processed_urls:
                    self.processed_urls.add(full_url)
                    
                    title = self._extract_title_near_link_fast(html, match)
                    pdf_links.append({
                        'url': full_url,
                        'title': title,
                        'filename': os.path.basename(match),
                        'found_on_page': base_url
                    })
        
        return pdf_links

    def _extract_title_near_link_fast(self, html: str, link: str) -> str:
        """‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß"""
        try:
            link_pos = html.find(link)
            if link_pos == -1:
                return "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠"
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î
            start = max(0, link_pos - 150)
            end = min(len(html), link_pos + 150)
            context = html[start:end]
            
            # ‡πÉ‡∏ä‡πâ pattern ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            patterns = [
                r'<title[^>]*>([^<]+)</title>',
                r'<h[1-6][^>]*>([^<]+)</h[1-6]>',
                r'>([^<]{15,80})<',
                r'alt=["\']([^"\']{10,})["\']'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, context, re.IGNORECASE)
                if match:
                    clean_title = re.sub(r'<[^>]+>', '', match.group(1)).strip()
                    if len(clean_title) > 10 and 'pdf' not in clean_title.lower():
                        return clean_title[:100]
            
            return f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PDF ‡∏à‡∏≤‡∏Å {urlparse(link).netloc}"
            
        except Exception:
            return "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠"

    async def _find_navigation_links(self, html: str, base_url: str) -> List[str]:
        """‡∏´‡∏≤‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏ô‡∏≥‡∏ó‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß"""
        nav_links = []
        
        # ‡πÉ‡∏ä‡πâ pattern ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        patterns = [
            r'href=["\']([^"\']*(?:page|‡∏´‡∏ô‡πâ‡∏≤|next|category|archive)[^"\']*)["\']',
            r'href=["\']([^"\']*\/\d+\/?)["\']',
            r'href=["\']([^"\']*\?page[^"\']*)["\']'
        ]
        
        base_netloc = urlparse(base_url).netloc
        
        for pattern in patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            for match in matches:
                if match.startswith('http'):
                    full_url = match
                else:
                    full_url = urljoin(base_url, match)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö domain ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                if urlparse(full_url).netloc == base_netloc:
                    nav_links.append(full_url)
        
        return list(set(nav_links))

    def _remove_duplicate_links_fast(self, links: List[Dict]) -> List[Dict]:
        """‡∏•‡∏ö‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏ã‡πâ‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß"""
        seen_urls = set()
        unique_links = []
        
        for link in links:
            url = link['url']
            if url not in seen_urls:
                seen_urls.add(url)
                unique_links.append(link)
        
        return unique_links

    async def extract_pdf_text_fast(self, pdf_url: str) -> tuple:
        """‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏•‡∏∞ metadata ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô"""
        if not self.ocr:
            return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ OCR ‡πÑ‡∏î‡πâ", {'content_length': 'N/A', 'content_type': 'N/A'}
        
        try:
            # ‡πÉ‡∏ä‡πâ OCR ‡∏Å‡∏±‡∏ö URL ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            text_task = self.ocr.extract_text_from_pdf_url_cached(pdf_url)
            
            # ‡∏î‡∏∂‡∏á metadata
            async def get_metadata():
                try:
                    timeout = aiohttp.ClientTimeout(total=10)
                    async with aiohttp.ClientSession(timeout=timeout, headers=self.fallback_headers) as session:
                        async with session.head(pdf_url) as response:
                            return {
                                'content_length': response.headers.get('content-length', 'N/A'),
                                'content_type': response.headers.get('content-type', 'N/A'),
                                'status': response.status
                            }
                except:
                    return {'content_length': 'N/A', 'content_type': 'N/A', 'status': 'Error'}
            
            # ‡∏£‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
            text, metadata = await asyncio.gather(text_task, get_metadata(), return_exceptions=True)
            
            if isinstance(text, Exception):
                text = f"OCR Error: {str(text)}"
            if isinstance(metadata, Exception):
                metadata = {'content_length': 'N/A', 'content_type': 'N/A', 'status': 'Error'}
            
            return text, metadata
            
        except Exception as e:
            return f"Error: {str(e)}", {'content_length': 'N/A', 'content_type': 'N/A', 'status': 'Error'}

    async def process_single_pdf_fast(self, pdf_info: Dict, index: int, pdf_limit: int = 0) -> List[Dict]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PDF ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß)"""
        try:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô limit ‡πÉ‡∏´‡πâ return None
            if pdf_limit > 0 and index > pdf_limit:
                return []

            text, metadata = await self.extract_pdf_text_fast(pdf_info['url'])

            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metadata
            original_text = text
            is_ocr_success = text and len(text.strip()) > 20 and not text.startswith("OCR Error")
            
            if is_ocr_success:
                cleaned_text = await self._clean_extracted_text_fast(text)
            else:
                cleaned_text = f"OCR Error: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ"

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á enhanced metadata
            enhanced_metadata = self._generate_enhanced_metadata(
                pdf_info, cleaned_text, original_text, metadata, is_ocr_success
            )

            # ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô 30,000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÉ‡∏´‡πâ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢ rows
            max_length = 30000
            results = []
            
            if len(cleaned_text) <= max_length:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 30,000 ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á row ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
                results.append({
                    'id': index,
                    'url': pdf_info['url'],
                    'title': pdf_info['title'],
                    'text': cleaned_text,
                    'filename': pdf_info['filename'],
                    'found_on_page': pdf_info['found_on_page'],
                    'file_size': metadata.get('content_length', 'N/A'),
                    'content_type': metadata.get('content_type', 'N/A'),
                    'created_at': datetime.now().isoformat(),
                    'metadata': enhanced_metadata
                })
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô 30,000 ‡πÉ‡∏´‡πâ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢ rows
                text_chunks = self._split_text_smartly(cleaned_text, max_length)
                logger.info(f"üìÑ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß {len(cleaned_text):,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏õ‡πá‡∏ô {len(text_chunks)} ‡∏™‡πà‡∏ß‡∏ô")
                
                for chunk_index, chunk_text in enumerate(text_chunks):
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk
                    chunk_metadata = enhanced_metadata.copy()
                    chunk_metadata['text_analysis']['is_chunk'] = True
                    chunk_metadata['text_analysis']['chunk_index'] = chunk_index + 1
                    chunk_metadata['text_analysis']['total_chunks'] = len(text_chunks)
                    chunk_metadata['text_analysis']['character_count'] = len(chunk_text)
                    chunk_metadata['text_analysis']['word_count'] = len(chunk_text.split())
                    
                    results.append({
                        'id': f"{index}.{chunk_index + 1}",  # ‡πÄ‡∏ä‡πà‡∏ô 1.1, 1.2, 1.3
                        'url': pdf_info['url'],
                        'title': f"{pdf_info['title']} (‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà {chunk_index + 1}/{len(text_chunks)})",
                        'text': chunk_text,
                        'filename': pdf_info['filename'],
                        'found_on_page': pdf_info['found_on_page'],
                        'file_size': metadata.get('content_length', 'N/A'),
                        'content_type': metadata.get('content_type', 'N/A'),
                        'created_at': datetime.now().isoformat(),
                        'metadata': chunk_metadata
                    })

            return results

        except Exception as e:
            logger.error(f"‚ùå Error processing {pdf_info['url']}: {e}")
            return []

    async def _clean_extracted_text_fast(self, text: str) -> str:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û"""
        if not text:
            return ""
        
        # 1. ‡∏•‡∏ö markdown elements ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "text" ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÅ‡∏ö‡∏ö‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á)
        text = re.sub(r'!\[.*?\]\(.*?\)', '', text)  # ‡∏•‡∏ö image references
        text = re.sub(r'\[.*?\]\(.*?\)', '', text)   # ‡∏•‡∏ö links
        text = re.sub(r'#{1,6}\s*', '', text)       # ‡∏•‡∏ö headers ‡πÅ‡∏ï‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        
        # ‡∏•‡∏ö‡∏Ñ‡∏≥ "text" ‡πÅ‡∏ö‡∏ö‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á (‡πÑ‡∏°‡πà‡∏•‡∏ö context, texture etc.)
        text = re.sub(r'\btext\b', '', text, flags=re.IGNORECASE)  # ‡∏•‡∏ö‡∏Ñ‡∏≥ "text" ‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡πÜ
        text = re.sub(r'^\s*text\s*$', '', text, flags=re.IGNORECASE | re.MULTILINE)  # ‡∏•‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏Ñ‡πà "text"
        text = re.sub(r'\s+text\s+', ' ', text, flags=re.IGNORECASE)  # ‡∏•‡∏ö "text" ‡∏ó‡∏µ‡πà‡∏°‡∏µ space ‡∏£‡∏≠‡∏ö
        
        # 2. ‡∏•‡∏ö HTML tags ‡πÅ‡∏•‡∏∞ entities
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)
        
        # 3. ‡∏•‡∏ö table formatting ‡πÅ‡∏•‡∏∞ special characters ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        text = re.sub(r'\|+', ' ', text)            # ‡∏•‡∏ö table separators
        text = re.sub(r':--+:?', ' ', text)         # ‡∏•‡∏ö table alignment
        text = re.sub(r'\$+.*?\$+', ' ', text)      # ‡∏•‡∏ö LaTeX math
        text = re.sub(r'\\[a-zA-Z]+\{.*?\}', ' ', text)  # ‡∏•‡∏ö LaTeX commands
        
        # 4. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ whitespace ‡πÅ‡∏•‡∏∞ newlines
        text = re.sub(r'\n+', ' ', text)            # ‡πÅ‡∏õ‡∏•‡∏á newlines ‡πÄ‡∏õ‡πá‡∏ô spaces
        text = re.sub(r'\s+', ' ', text)            # ‡∏£‡∏ß‡∏° multiple spaces
        text = text.strip()
        
        # 4.5. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏•‡∏Ç‡πÄ‡∏ó‡∏ß‡∏ô‡∏≤‡∏Ñ‡∏£‡∏µ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢
        devanagari_to_thai = str.maketrans('‡•¶‡•ß‡•®‡•©‡•™‡•´‡•¨‡•≠‡•Æ‡•Ø', '‡πê‡πë‡πí‡πì‡πî‡πï‡πñ‡πó‡πò‡πô')
        text = text.translate(devanagari_to_thai)
        
        # 4.6. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà OCR ‡∏≠‡πà‡∏≤‡∏ô‡∏ú‡∏¥‡∏î (‡πÄ‡∏ä‡πà‡∏ô ‡∏∞ ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô ‡πñ) - ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        
        # Pattern-based fixes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏õ‡∏µ ‡∏û.‡∏®. ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î
        text = re.sub(r'(\d+)‡∏∞(\d+)', r'\1‡πñ\2', text)  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ‡∏∞ ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏•‡∏Ç‡πÄ‡∏õ‡πá‡∏ô ‡πñ
        text = re.sub(r'‡∏û\.‡∏®\.\s*‡πí‡πï‡∏∞([‡πê-‡πô])', r'‡∏û.‡∏®. ‡πí‡πï‡πñ\1', text)  # ‡πÅ‡∏Å‡πâ‡∏õ‡∏µ ‡∏û.‡∏®. ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        
        # Common OCR fixes
        common_ocr_fixes = {
            # ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢
            '‡πí‡πï‡∏∞‡πò': '‡πí‡πï‡πñ‡πò', '‡πí‡πï‡∏∞‡πó': '‡πí‡πï‡πñ‡πó', '‡πí‡πï‡∏∞‡πô': '‡πí‡πï‡πñ‡πô', '‡πí‡πï‡∏∞‡πê': '‡πí‡πï‡πñ‡πê',
            '‡πí‡πï‡∏∞‡πë': '‡πí‡πï‡πñ‡πë', '‡πí‡πï‡∏∞‡πí': '‡πí‡πï‡πñ‡πí', '‡πí‡πï‡∏∞‡πì': '‡πí‡πï‡πñ‡πì', '‡πí‡πï‡∏∞‡πî': '‡πí‡πï‡πñ‡πî', '‡πí‡πï‡∏∞‡πï': '‡πí‡πï‡πñ‡πï',
            # ‡πÄ‡∏•‡∏Ç‡∏°‡∏≤‡∏ï‡∏£‡∏≤
            '‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡∏∞': '‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡πñ', '‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡πë‡∏∞': '‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡πë‡πñ', '‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡πí‡∏∞': '‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡πí‡πñ',
            # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
            '‡∏∞‡πê': '‡πñ‡πê', '‡∏∞‡πë': '‡πñ‡πë', '‡∏∞‡πí': '‡πñ‡πí', '‡∏∞‡πì': '‡πñ‡πì', '‡∏∞‡πî': '‡πñ‡πî', '‡∏∞‡πï': '‡πñ‡πï',
            '‡∏∞‡πó': '‡πñ‡πó', '‡∏∞‡πò': '‡πñ‡πò', '‡∏∞‡πô': '‡πñ‡πô',
        }
        for wrong, correct in common_ocr_fixes.items():
            text = text.replace(wrong, correct)
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ ‡∏û.‡∏®. ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        # ‡∏´‡∏≤‡∏õ‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô title ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡πâ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        year_match = re.search(r'‡∏û\.‡∏®\.\s*(\d{4})', text[:500])
        if year_match:
            main_year = year_match.group(1)
            # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏õ‡∏µ‡∏´‡∏•‡∏±‡∏Å (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
            text = re.sub(r'‡∏û\.‡∏®\.\s*\d{4}', f'‡∏û.‡∏®. {main_year}', text)
        
        logger.debug("üî¢ ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏•‡∏Ç‡πÄ‡∏ó‡∏ß‡∏ô‡∏≤‡∏Ñ‡∏£‡∏µ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç OCR ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡πâ‡∏ß")
        
        # 5. ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡πÅ‡∏ï‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏£‡∏∞‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô
        # ‡πÄ‡∏Å‡πá‡∏ö: ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÑ‡∏ó‡∏¢
        text = re.sub(r'[^\w\s\.\,\!\?\-\(\)\:\;\"\'‡πê-‡πô\u0E00-\u0E7F]', '', text, flags=re.UNICODE)
        
        # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
        if len(text) < 20:
            return f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô: {text}"
        
        # 7. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) - Smart Error Detection + Fast Correction
        if PYTHAINLP_AVAILABLE and len(text) > 50:
            try:
                # üéØ Step 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å)
                potential_errors = []
                
                # Pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô OCR
                error_patterns = [
                    r'[‡πê-‡πô‡•¶-‡•Ø]+[‡∏∞][‡πê-‡πô‡•¶-‡•Ø]+',  # ‡πÄ‡∏ä‡πà‡∏ô "‡πí‡πï‡∏∞‡πò" ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô "‡πí‡πï‡πñ‡πò"
                    r'[‡πê-‡πô‡•¶-‡•Ø]+[,][‡πê-‡πô‡•¶-‡•Ø]+[‡•¶][‡πê-‡πô‡•¶-‡•Ø]+[,][‡•¶]+',  # ‡πÄ‡∏•‡∏Ç‡∏ú‡∏™‡∏° ‡πÄ‡∏ó‡∏ß‡∏ô‡∏≤‡∏Ñ‡∏£‡∏µ ‡πÄ‡∏ä‡πà‡∏ô "‡πì,‡πó‡πï‡πñ,‡πó‡πê‡πê,‡πê‡πê‡πê,‡•¶‡•¶‡•¶"
                    r'[‡•¶-‡•Ø]+',  # ‡πÄ‡∏•‡∏Ç‡πÄ‡∏ó‡∏ß‡∏ô‡∏≤‡∏Ñ‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢
                    r'[‡∏Å-‡πô]{1,3}[‡∏Å‡∏µ‡πà]{1,2}[‡∏Å-‡πô]{1,3}',  # ‡πÄ‡∏ä‡πà‡∏ô "‡∏Å‡∏µ‡πà" ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô "‡∏Å‡∏±‡∏ô"
                    r'[‡πê-‡πô]+[‡∏ï‡∏ö‡∏õ‡∏ú‡∏ù‡∏û‡∏ü‡∏†‡∏°]\b',  # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ú‡∏¥‡∏î
                    r'\b[‡∏ï‡∏ö‡∏õ‡∏ú‡∏ù‡∏û‡∏ü‡∏†‡∏°]{1,2}\b',  # ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏ú‡∏¥‡∏î‡πÜ
                    r'[‡∏Å-‡∏Æ]{10,}',     # ‡∏Ñ‡∏≥‡∏¢‡∏≤‡∏ß‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
                    r'\.{3,}',         # ‡∏à‡∏∏‡∏î‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    r'\s{3,}',         # space ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                ]
                
                # ‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
                error_spans = []
                for pattern in error_patterns:
                    for match in re.finditer(pattern, text):
                        start, end = match.span()
                        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏£‡∏≠‡∏ö‡πÜ (‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á 15 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤-‡∏´‡∏•‡∏±‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
                        context_start = max(0, start - 15)
                        context_end = min(len(text), end + 15)
                        error_spans.append((context_start, context_end, match.group()))
                
                # üöÄ Step 2: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å)
                if error_spans:
                    logger.info(f"üîç ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î {len(error_spans)} ‡∏à‡∏∏‡∏î - ‡∏à‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ")
                    
                    corrected_parts = {}
                    corrected_count = 0
                    
                    for i, (start, end, error_text) in enumerate(error_spans[:3]):  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÅ‡∏Ñ‡πà 3 ‡∏à‡∏∏‡∏î‡πÅ‡∏£‡∏Å (‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å)
                        context = text[start:end]
                        
                        # ‡∏•‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
                        if len(context) > 50:
                            context = context[:50]
                        
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö cache
                        context_hash = hashlib.md5(context.encode()).hexdigest()[:8]
                        if context_hash in _pythainlp_cache:
                            corrected_parts[(start, end)] = _pythainlp_cache[context_hash]
                            logger.debug(f"üìö Cache hit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö error {i+1}")
                            continue
                        
                        # üéØ Step 2.1: ‡πÉ‡∏ä‡πâ PyThaiNLP ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î (‡∏î‡πâ‡∏ß‡∏¢ timeout ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô)
                        start_time = time.time()
                        try:
                            # ‡πÉ‡∏ä‡πâ executor ‡πÄ‡∏û‡∏∑‡πà‡∏≠ timeout PyThaiNLP - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ callable
                            import concurrent.futures
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ pythainlp.correct.correct ‡πÅ‡∏ó‡∏ô correct ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                                future = executor.submit(pythainlp.correct.correct, context)
                                try:
                                    pythainlp_corrected = future.result(timeout=2.0)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡πâ PyThaiNLP ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
                                except concurrent.futures.TimeoutError:
                                    logger.warning(f"‚è∞ Error {i+1} PyThaiNLP timeout ‡∏´‡∏•‡∏±‡∏á 2.0s, ‡∏Ç‡πâ‡∏≤‡∏°")
                                    break
                            
                            elapsed = time.time() - start_time
                            
                            # ‡∏ñ‡πâ‡∏≤ PyThaiNLP ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏î‡πâ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ä‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                            if pythainlp_corrected and pythainlp_corrected != context:
                                # ‡πÉ‡∏ä‡πâ PyThaiNLP ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ DeepSeek ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
                                if hasattr(self, 'ocr') and self.ocr and self.ocr.deepseek_client:
                                    # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà PyThaiNLP ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏´‡πâ DeepSeek ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
                                    import asyncio
                                    deepseek_corrected = await self.ocr.correct_text_with_deepseek(context)
                                    if deepseek_corrected != context:
                                        corrected_parts[(start, end)] = deepseek_corrected
                                        _pythainlp_cache[context_hash] = deepseek_corrected
                                        corrected_count += 1
                                        logger.info(f"ü§ñ PyThaiNLP‚ÜíDeepSeek ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç error {i+1}: '{error_text[:15]}...'")
                                        continue
                                
                                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ DeepSeek ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ PyThaiNLP ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
                                corrected_parts[(start, end)] = pythainlp_corrected
                                _pythainlp_cache[context_hash] = pythainlp_corrected
                                corrected_count += 1
                                logger.info(f"üìù PyThaiNLP ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç error {i+1}/{len(error_spans)} ({elapsed:.1f}s): '{error_text[:15]}...'")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Error {i+1} PyThaiNLP ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
                    
                    # üîß Step 3: ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                    if corrected_parts:
                        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ index ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô
                        for (start, end), corrected_text in sorted(corrected_parts.items(), reverse=True):
                            text = text[:start] + corrected_text + text[end:]
                        
                        logger.info(f"üéâ PyThaiNLP: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç {corrected_count}/{len(error_spans)} ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                    else:
                        logger.info("üìù PyThaiNLP: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏î‡πâ")
                else:
                    logger.info("‚úÖ PyThaiNLP: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç")
                
                # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏ô‡∏≤‡∏î cache
                if len(_pythainlp_cache) > 200:
                    _pythainlp_cache.clear()
                    logger.debug("üßπ PyThaiNLP cache cleared")
                        
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è PyThaiNLP smart correction failed: {e}")
        
        # 8. ‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏Ñ‡∏£‡∏ö
        return text

    def _generate_enhanced_metadata(self, pdf_info: Dict, cleaned_text: str, original_text: str, 
                                  metadata: Dict, is_ocr_success: bool) -> Dict:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á metadata ‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dataset"""
        try:
            # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            file_size_bytes = metadata.get('content_length', 'N/A')
            if file_size_bytes != 'N/A' and str(file_size_bytes).isdigit():
                file_size_mb = round(int(file_size_bytes) / (1024 * 1024), 2)
            else:
                file_size_mb = 'N/A'

            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            word_count = len(cleaned_text.split()) if cleaned_text else 0
            char_count = len(cleaned_text) if cleaned_text else 0
            line_count = len(cleaned_text.split('\n')) if cleaned_text else 0

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
            doc_type = self._classify_document_type(cleaned_text)
            
            # ‡∏î‡∏∂‡∏á‡∏õ‡∏µ ‡∏û.‡∏®. ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            buddhist_year = self._extract_buddhist_year(cleaned_text)
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            keywords = self._extract_keywords(cleaned_text)

            # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• URL
            url_parts = urlparse(pdf_info['url'])
            document_id = self._extract_document_id(pdf_info['filename'])

            return {
                'processing': {
                    'ocr_success': is_ocr_success,
                    'ocr_engine': 'mistral-ocr-latest',
                    'text_correction': 'pythainlp+deepseek' if (PYTHAINLP_AVAILABLE and self.ocr and self.ocr.deepseek_client) else ('pythainlp' if PYTHAINLP_AVAILABLE else 'none'),
                    'processing_timestamp': datetime.now().isoformat(),
                    'text_length_original': len(original_text) if original_text else 0,
                    'text_length_cleaned': char_count,
                    'has_markdown': '![' in original_text if original_text else False
                },
                'file_info': {
                    'file_size_bytes': file_size_bytes,
                    'file_size_mb': file_size_mb,
                    'content_type': metadata.get('content_type', 'N/A'),
                    'document_id': document_id,
                    'url_domain': url_parts.netloc,
                    'url_path': url_parts.path
                },
                'text_analysis': {
                    'word_count': word_count,
                    'character_count': char_count,
                    'line_count': line_count,
                    'avg_words_per_line': round(word_count / line_count, 2) if line_count > 0 else 0,
                    'document_type': doc_type,
                    'buddhist_year': buddhist_year,
                    'keywords': keywords[:10]  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 10 ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å
                },
                'source': {
                    'base_url': 'https://ratchakitcha.soc.go.th',
                    'discovered_on': pdf_info.get('found_on_page', 'N/A'),
                    'scraper_version': '1.0.0',
                    'scraper_method': 'crawl4ai'
                }
            }
        except Exception as e:
            logger.warning(f"Error generating metadata: {e}")
            return {
                'processing': {'ocr_success': is_ocr_success, 'error': str(e)},
                'file_info': {'file_size_bytes': file_size_bytes},
                'text_analysis': {'word_count': word_count, 'character_count': char_count},
                'source': {'base_url': 'https://ratchakitcha.soc.go.th'}
            }

    def _classify_document_type(self, text: str) -> str:
        """‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"""
        if not text:
            return "unknown"
        
        text_lower = text.lower()
        
        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏£‡∏≤‡∏ä‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏ô‡∏∏‡πÄ‡∏ö‡∏Å‡∏©‡∏≤
        if any(word in text_lower for word in ['‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥', 'royal decree']):
            return "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥"
        elif any(word in text_lower for word in ['‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', 'announcement']):
            return "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®"
        elif any(word in text_lower for word in ['‡∏Å‡∏é‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á', 'ministerial regulation']):
            return "‡∏Å‡∏é‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á"
        elif any(word in text_lower for word in ['‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á', 'order']):
            return "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á"
        elif any(word in text_lower for word in ['‡∏Ç‡πâ‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö', 'regulation']):
            return "‡∏Ç‡πâ‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö"
        elif any(word in text_lower for word in ['‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á', 'appointment']):
            return "‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á"
        elif any(word in text_lower for word in ['‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì', 'budget']):
            return "‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì"
        else:
            return "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"

    def _extract_buddhist_year(self, text: str) -> Optional[str]:
        """‡∏î‡∏∂‡∏á‡∏õ‡∏µ ‡∏û.‡∏®. ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        if not text:
            return None
        
        # ‡∏´‡∏≤ pattern ‡∏õ‡∏µ ‡∏û.‡∏®.
        patterns = [
            r'‡∏û\.‡∏®\.?\s*(\d{4})',
            r'‡∏û‡∏∏‡∏ó‡∏ò‡∏®‡∏±‡∏Å‡∏£‡∏≤‡∏ä\s*(\d{4})',
            r'B\.E\.?\s*(\d{4})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                year = match.group(1)
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏µ ‡∏û.‡∏®. ‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏• (2400-2600)
                if 2400 <= int(year) <= 2600:
                    return year
        
        return None

    def _extract_keywords(self, text: str) -> List[str]:
        """‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        if not text:
            return []
        
        # ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏£‡∏≤‡∏ä‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏ô‡∏∏‡πÄ‡∏ö‡∏Å‡∏©‡∏≤
        important_keywords = [
            '‡∏û‡∏£‡∏∞‡∏ö‡∏≤‡∏ó‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡πÄ‡∏à‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏±‡∏ß', '‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ', '‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ', '‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á',
            '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', '‡∏Å‡∏é‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á', '‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì', '‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á',
            '‡∏£‡∏≤‡∏ä‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏ô‡∏∏‡πÄ‡∏ö‡∏Å‡∏©‡∏≤', '‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç', '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏Å‡∏§‡∏©‡∏é‡∏µ‡∏Å‡∏≤'
        ]
        
        found_keywords = []
        text_lower = text.lower()
        
        for keyword in important_keywords:
            if keyword.lower() in text_lower:
                found_keywords.append(keyword)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢ (simple approach)
        words = re.findall(r'\b\w{4,}\b', text)  # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 4 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
        word_freq = {}
        for word in words:
            if len(word) >= 4:  # ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏¢‡∏≤‡∏ß‡πÜ
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # ‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢‡∏™‡∏∏‡∏î 5 ‡∏Ñ‡∏≥
        frequent_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        found_keywords.extend([word for word, freq in frequent_words if freq > 1])
        
        return list(set(found_keywords))  # ‡∏•‡∏ö‡∏ã‡πâ‡∏≥

    def _extract_document_id(self, filename: str) -> Optional[str]:
        """‡∏î‡∏∂‡∏á‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå"""
        if not filename:
            return None
        
        # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        match = re.search(r'(\d+)', filename)
        if match:
            return match.group(1)
        
        return None

    def _split_text_smartly(self, text: str, max_length: int) -> List[str]:
        """‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏≤‡∏ç‡∏â‡∏•‡∏≤‡∏î ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥"""
        if len(text) <= max_length:
            return [text]
        
        chunks = []
        current_pos = 0
        
        while current_pos < len(text):
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î
            end_pos = min(current_pos + max_length, len(text))
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
            if end_pos < len(text):
                # ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏µ ‡πÄ‡∏ä‡πà‡∏ô ‡∏à‡∏∏‡∏î, ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà, ‡∏ß‡∏£‡∏£‡∏Ñ
                good_break_points = [
                    text.rfind('\n\n', current_pos, end_pos),  # paragraph break
                    text.rfind('. ', current_pos, end_pos),    # sentence end
                    text.rfind('‡•§ ', current_pos, end_pos),    # Thai sentence end
                    text.rfind('\n', current_pos, end_pos),    # line break
                    text.rfind(' ', current_pos, end_pos),     # word boundary
                ]
                
                # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                best_break = -1
                for break_point in good_break_points:
                    if break_point > current_pos + max_length * 0.7:  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 70% ‡∏Ç‡∏≠‡∏á max_length
                        best_break = break_point
                        break
                
                if best_break > current_pos:
                    end_pos = best_break + 1  # +1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏° delimiter
            
            # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            chunk = text[current_pos:end_pos].strip()
            if chunk:
                chunks.append(chunk)
            
            current_pos = end_pos
        
        return chunks

    async def create_dataset_ultra_fast(self, pdf_links: List[Dict], concurrent_limit: int = 15) -> List[Dict]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß)"""
        logger.info(f"üöÄ ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏à‡∏≤‡∏Å {len(pdf_links)} ‡πÑ‡∏ü‡∏•‡πå (crawl4ai + Windows, concurrent: {concurrent_limit})")
        
        progress = ProgressTracker(len(pdf_links))
        dataset = []
        
        # ‡∏•‡∏î concurrent ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows stability
        semaphore = asyncio.Semaphore(min(concurrent_limit, 12))
        
        async def process_with_semaphore(pdf_info, index):
            async with semaphore:
                result = await self.process_single_pdf_fast(pdf_info, index + 1)
                progress.update(success=len(result) > 0 if result else False)
                return result
        
        tasks = [process_with_semaphore(pdf_info, i) for i, pdf_info in enumerate(pdf_links)]
        
        for coro in asyncio.as_completed(tasks):
            try:
                result = await coro
                if result:  # result ‡πÄ‡∏õ‡πá‡∏ô List[Dict]
                    dataset.extend(result)  # ‡πÉ‡∏ä‡πâ extend ‡πÅ‡∏ó‡∏ô append
                
                if len(dataset) % 20 == 0:
                    gc.collect()
                    
            except Exception as e:
                logger.error(f"Task failed: {e}")
                progress.update(success=False)
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° id (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á int ‡πÅ‡∏•‡∏∞ string)
        def sort_key(x):
            id_val = x['id']
            if isinstance(id_val, str) and '.' in id_val:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö id ‡πÅ‡∏ö‡∏ö "1.1", "1.2" 
                parts = id_val.split('.')
                return (int(parts[0]), int(parts[1]))
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö id ‡πÅ‡∏ö‡∏ö int
                return (int(id_val), 0)
        
        dataset.sort(key=sort_key)
        
        elapsed = time.time() - progress.start_time
        total_chunks = sum(1 for item in dataset if item.get('metadata', {}).get('text_analysis', {}).get('is_chunk', False))
        
        logger.info(f"üéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! {len(dataset)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {elapsed/60:.1f} ‡∏ô‡∏≤‡∏ó‡∏µ "
                   f"(‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ {len(dataset)/elapsed:.1f} ‡πÑ‡∏ü‡∏•‡πå/‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)")
        
        if total_chunks > 0:
            logger.info(f"üìÑ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏£‡∏ß‡∏° {total_chunks} chunks ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏¥‡πâ‡∏ô")
        
        return dataset

    async def save_dataset_to_csv(self, dataset: List[Dict], filename: str = "ratchakitcha_dataset.csv"):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Dataset ‡πÄ‡∏õ‡πá‡∏ô CSV"""
        if not dataset:
            logger.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
            return
        
        try:
            df = pd.DataFrame(dataset)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            logger.info(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Dataset ‡∏•‡∏á {filename} ({len(dataset)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
        except Exception as e:
            logger.error(f"‚ùå Error ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å CSV: {e}")

    async def save_dataset_to_json(self, dataset: List[Dict], filename: str = "ratchakitcha_dataset.json"):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Dataset ‡πÄ‡∏õ‡πá‡∏ô JSON"""
        try:
            async with aiofiles.open(filename, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(dataset, ensure_ascii=False, indent=2))
            logger.info(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Dataset ‡∏•‡∏á {filename}")
        except Exception as e:
            logger.error(f"‚ùå Error ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å JSON: {e}")

async def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å - crawl4ai Fixed for Windows + DeepSeek Support"""
    print("üöÄ === Ratchakitcha PDF Scraper - MULTI OCR SUPPORT ===")
    print("ü™ü ‡πÉ‡∏ä‡πâ crawl4ai ‡πÅ‡∏ï‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Windows Python 3.13")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ APIs
    print("\nüîó API Status:")
    mistral_key = os.getenv("MISTRAL_API_KEY")
    deepseek_key = os.getenv("DEEPSEEK_API_KEY")
    
    if deepseek_key and DEEPSEEK_AVAILABLE:
        print("üíé DeepSeek API: ‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î 70‡πÄ‡∏ó‡πà‡∏≤!)")
    else:
        print("üíé DeepSeek API: ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    
    if mistral_key:
        print("ü§ñ Mistral API: ‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    else:
        print("ü§ñ Mistral API: ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ PyThaiNLP
    global PYTHAINLP_AVAILABLE
    if PYTHAINLP_AVAILABLE:
        print(f"üìù PyThaiNLP: ‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏°‡∏µ cache ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡πâ‡∏≤‡∏á)")
        try:
            use_pythainlp = input("\nü§î ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ PyThaiNLP ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î? (y/N, ‡∏Å‡∏î Enter = ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ): ").strip().lower()
            if use_pythainlp not in ['y', 'yes']:
                PYTHAINLP_AVAILABLE = False
                print("‚ö†Ô∏è ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô PyThaiNLP (‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô)")
            else:
                print("‚úÖ ‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô PyThaiNLP")
        except:
            PYTHAINLP_AVAILABLE = False
            print("‚ö†Ô∏è ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô PyThaiNLP (‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô)")
    else:
        print(f"üìù PyThaiNLP: ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    
    print()
    scraper = RatchakitchaScraperCrawl4AI()
    
    if not scraper.ocr:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô OCR - ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Keys")
        if not mistral_key and not deepseek_key:
            print("üí° ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ MISTRAL_API_KEY ‡∏´‡∏£‡∏∑‡∏≠ DEEPSEEK_API_KEY")
        return
    
    # ‡πÅ‡∏™‡∏î‡∏á OCR ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    if scraper.ocr.use_deepseek_correction:
        print("üéØ ‡πÉ‡∏ä‡πâ Mistral OCR + DeepSeek Text Correction (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î 70‡πÄ‡∏ó‡πà‡∏≤!)")
    else:
        print("üéØ ‡πÉ‡∏ä‡πâ Mistral OCR + Pattern-based Text Correction")
    
    if not scraper.use_crawl4ai:
        print("‚ö†Ô∏è crawl4ai ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ aiohttp fallback")
    
    try:
        start_time = time.time()
        
        # 1. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå PDF
        print("üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå PDF (crawl4ai Windows Fixed)...")
        pdf_links = await scraper.discover_pdf_links_crawl4ai(max_pages=30)
        
        if not pdf_links:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå PDF")
            return
        
        discovery_time = time.time() - start_time
        print(f"‚úÖ ‡∏û‡∏ö {len(pdf_links)} ‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {discovery_time:.1f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        print("\nüìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö:")
        for i, pdf in enumerate(pdf_links[:5], 1):
            print(f"   {i}. {pdf['title'][:50]}...")
            print(f"      URL: {pdf['url']}")
            print(f"      ‡πÑ‡∏ü‡∏•‡πå: {pdf['filename']}")
        
        if len(pdf_links) > 5:
            print(f"   ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(pdf_links) - 5} ‡πÑ‡∏ü‡∏•‡πå")
        
        # 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
        try:
            concurrent_input = input(f"\nConcurrent limit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: 10-15 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows, default: 12): ").strip()
            concurrent_limit = int(concurrent_input) if concurrent_input.isdigit() else 12
            concurrent_limit = min(concurrent_limit, 15)
        except:
            concurrent_limit = 12
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå
        try:
            limit_input = input(f"\nüìä ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(pdf_links)} ‡πÑ‡∏ü‡∏•‡πå, ‡∏Å‡∏î Enter = ‡∏ó‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î): ").strip()
            if limit_input.isdigit() and int(limit_input) > 0:
                file_limit = min(int(limit_input), len(pdf_links))
                pdf_links = pdf_links[:file_limit]
                print(f"üìù ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á {file_limit} ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å")
            else:
                print(f"üìù ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(pdf_links)} ‡πÑ‡∏ü‡∏•‡πå")
        except:
            print(f"üìù ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(pdf_links)} ‡πÑ‡∏ü‡∏•‡πå")
        
        print(f"üöÄ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ {concurrent_limit} concurrent connections (Windows Safe)")
        
        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset
        dataset = await scraper.create_dataset_ultra_fast(pdf_links, concurrent_limit=concurrent_limit)
        
        if dataset:
            # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
            save_tasks = [
                scraper.save_dataset_to_csv(dataset, "ratchakitcha_dataset.csv"),
                scraper.save_dataset_to_json(dataset, "ratchakitcha_dataset.json")
            ]
            await asyncio.gather(*save_tasks)
            
            total_time = time.time() - start_time
            print(f"\nüéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô crawl4ai Scraping!")
            print(f"‚è±Ô∏è  ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_time/60:.1f} ‡∏ô‡∏≤‡∏ó‡∏µ")
            print(f"üìä ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û: {len(dataset)/total_time:.1f} ‡πÑ‡∏ü‡∏•‡πå/‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
            print(f"üìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á: ratchakitcha_dataset.csv, .json ({len(dataset)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
            print(f"\nüìÑ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
            for i, item in enumerate(dataset[:2], 1):
                print(f"\n{i}. {item['title'][:60]}...")
                print(f"   ‡∏Ç‡∏ô‡∏≤‡∏î: {item['file_size']} bytes")
                if len(item['text']) > 100:
                    print(f"   ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {item['text'][:80]}...")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")

if __name__ == "__main__":
    asyncio.run(main())
