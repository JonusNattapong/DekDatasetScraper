import asyncio
import json
import csv
import pandas as pd
from urllib.parse import urljoin, urlparse
import logging
from typing import List, Dict
import aiofiles
import time
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import re

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_user_pdf_limit():
    """‡∏£‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ"""
    try:
        limit = int(input("\n‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (0=‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î): ").strip() or "0")
        return max(0, limit)
    except ValueError:
        print("‚ö†Ô∏è ‡πÇ‡∏õ‡∏£‡∏î‡∏õ‡πâ‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")
        return 0

def get_user_pdf_limit():
    """‡∏£‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ"""
    try:
        limit = int(input("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (0=‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î): ").strip() or "0")
        return max(0, limit)
    except Exception:
        print("‚ö†Ô∏è ‡πÇ‡∏õ‡∏£‡∏î‡∏õ‡πâ‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")
        return 0

def get_user_input():
    """‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ"""
    print("\n===== ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• =====")
    try:
        limit = int(input("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (0=‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î): ").strip() or "0")
        return max(0, limit)
    except ValueError:
        print("‚ö†Ô∏è ‡πÇ‡∏õ‡∏£‡∏î‡∏õ‡πâ‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")
        return 0

class Crawl4AiDatasetScraper:
    def __init__(self, base_url: str = None):
        self.base_url = base_url or "https://ratchakitcha.soc.go.th"
        self.session_data = []
        self.dataset = []
        
        # Browser config for Crawl4AI
        self.browser_config = BrowserConfig(
            headless=True,
            verbose=True,
            browser_type="chromium"
        )
        
        # Crawler config
        self.run_config = CrawlerRunConfig(
            cache_mode=CacheMode.ENABLED,
            word_count_threshold=10,
            wait_for_images=True,
            delay_before_return_html=2.0,
            timeout=30
        )

    async def extract_links_from_page(self, url: str, link_patterns: List[str] = None) -> List[Dict]:
        """
        ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡πÅ‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Crawl4AI
        """
        links_data = []
        
        try:
            async with AsyncWebCrawler(config=self.browser_config) as crawler:
                logger.info(f"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏à‡∏≤‡∏Å: {url}")
                
                result = await crawler.arun(url=url, config=self.run_config)
                
                if result.success:
                    # ‡πÉ‡∏ä‡πâ regex ‡∏´‡∏≤‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡πÅ‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
                    if not link_patterns:
                        link_patterns = [
                            r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>([^<]+)</a>',
                            r'href=["\']([^"\']+)["\'][^>]*[^>]*>([^<]{10,})</a>'
                        ]
                    
                    # ‡∏´‡∏≤‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏à‡∏≤‡∏Å HTML
                    for pattern in link_patterns:
                        matches = re.findall(pattern, result.html, re.IGNORECASE | re.DOTALL)
                        for match in matches:
                            if len(match) >= 2:
                                link_url = match[0].strip()
                                title = re.sub(r'<[^>]+>', '', match[1]).strip()
                                
                                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                                if self._is_valid_link(link_url, title):
                                    full_url = urljoin(self.base_url, link_url)
                                    links_data.append({
                                        'url': full_url,
                                        'title': title
                                    })
                    
                    # ‡∏´‡∏≤‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏à‡∏≤‡∏Å links ‡∏ó‡∏µ‡πà Crawl4AI extract ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
                    if hasattr(result, 'links') and result.links:
                        internal_links = result.links.get('internal', [])
                        for link_info in internal_links:
                            if isinstance(link_info, dict):
                                link_url = link_info.get('href', '')
                                title = link_info.get('text', '').strip()
                            else:
                                continue
                            
                            if self._is_valid_link(link_url, title):
                                full_url = urljoin(self.base_url, link_url)
                                links_data.append({
                                    'url': full_url,
                                    'title': title
                                })
                    
                    logger.info(f"‚úÖ ‡∏û‡∏ö‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå {len(links_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                else:
                    logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {url}: {result.error_message}")
                    
        except Exception as e:
            logger.error(f"‚ùå Error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå: {str(e)}")
            
        return self._remove_duplicates(links_data)

    def _is_valid_link(self, url: str, title: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡πÅ‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        if not url or not title:
            return False
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        invalid_patterns = [
            r'^#', r'^javascript:', r'^mailto:', r'^tel:',
            r'\.(css|js|jpg|jpeg|png|gif|pdf|doc|docx)(\?|$)',
            r'/(login|logout|register|admin)/',
        ]
        
        for pattern in invalid_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        if len(title.strip()) < 5:
            return False
            
        return True

    def _remove_duplicates(self, links_data: List[Dict]) -> List[Dict]:
        """‡∏•‡∏ö‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏ã‡πâ‡∏≥"""
        seen = set()
        unique_links = []
        
        for link in links_data:
            identifier = (link['url'], link['title'])
            if identifier not in seen:
                seen.add(identifier)
                unique_links.append(link)
                
        return unique_links

    async def scrape_content_from_url(self, url: str, title: str = "") -> Dict:
        """
        ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å URL ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Dataset
        """
        try:
            async with AsyncWebCrawler(config=self.browser_config) as crawler:
                logger.info(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å: {url}")
                
                result = await crawler.arun(url=url, config=self.run_config)
                
                if result.success:
                    # ‡πÉ‡∏ä‡πâ markdown ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å
                    text_content = result.markdown or result.cleaned_html or ""
                    
                    # ‡∏´‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏°‡∏≤
                    if not title and hasattr(result, 'metadata'):
                        title = result.metadata.get('title', '') or self._extract_title_from_content(text_content)
                    
                    return {
                        'url': url,
                        'title': title.strip(),
                        'text': text_content.strip()
                    }
                else:
                    logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å {url}: {result.error_message}")
                    return None
                    
        except Exception as e:
            logger.error(f"‚ùå Error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ {url}: {str(e)}")
            return None

    def _extract_title_from_content(self, content: str) -> str:
        """‡πÅ‡∏¢‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤"""
        lines = content.split('\n')
        for line in lines[:10]:  # ‡∏î‡∏π‡πÅ‡∏Ñ‡πà 10 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å
            line = line.strip()
            if len(line) > 10 and len(line) < 200:
                return line
        return "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠"

    async def create_dataset_from_links(self, links_data: List[Dict], max_pages: int = 50) -> List[Dict]:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå
        """
        dataset = []
        processed = 0
        
        for i, link_info in enumerate(links_data[:max_pages], 1):
            try:
                content_data = await self.scrape_content_from_url(
                    link_info['url'], 
                    link_info['title']
                )
                
                if content_data and content_data['text']:
                    dataset.append({
                        'id': i,
                        'url': content_data['url'],
                        'title': content_data['title'],
                        'text': content_data['text']
                    })
                    processed += 1
                    logger.info(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß {processed}/{len(links_data[:max_pages])} ‡∏´‡∏ô‡πâ‡∏≤")
                
                # ‡∏´‡∏¢‡∏∏‡∏î‡∏û‡∏±‡∏Å‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏´‡∏ô‡∏±‡∏Å
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"‚ùå Error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {link_info['url']}: {str(e)}")
                continue
        
        logger.info(f"üéâ ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {len(dataset)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        return dataset

    async def save_dataset_to_csv(self, dataset: List[Dict], filename: str = "dataset.csv"):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Dataset ‡πÄ‡∏õ‡πá‡∏ô CSV"""
        try:
            if not dataset:
                logger.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
                return
            
            df = pd.DataFrame(dataset)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            logger.info(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Dataset ‡∏•‡∏á {filename} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß ({len(dataset)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")
            
        except Exception as e:
            logger.error(f"‚ùå Error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å CSV: {str(e)}")

    async def save_dataset_to_json(self, dataset: List[Dict], filename: str = "dataset.json"):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Dataset ‡πÄ‡∏õ‡πá‡∏ô JSON"""
        try:
            async with aiofiles.open(filename, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(dataset, ensure_ascii=False, indent=2))
            logger.info(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Dataset ‡∏•‡∏á {filename} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
            
        except Exception as e:
            logger.error(f"‚ùå Error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å JSON: {str(e)}")

    async def auto_scrape_website(self, start_url: str, max_pages: int = 20,
                                 keywords: List[str] = None) -> List[Dict]:
        """
        ‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset
        """
        logger.info(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô {start_url} ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥")
        
        # ‡∏£‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
        pdf_limit = get_user_pdf_limit()
        if pdf_limit > 0:
            logger.info(f"üìå ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {pdf_limit} ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å")
            max_pages = min(max_pages, pdf_limit)
        
        # 1. ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å
        links_data = await self.extract_links_from_page(start_url)
        
        # 2. ‡∏Å‡∏£‡∏≠‡∏á‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏ï‡∏≤‡∏° keywords ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        if keywords:
            filtered_links = []
            for link in links_data:
                for keyword in keywords:
                    if keyword.lower() in link['title'].lower() or keyword.lower() in link['url'].lower():
                        filtered_links.append(link)
                        break
            links_data = filtered_links
            logger.info(f"üîç ‡∏Å‡∏£‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ keywords ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(links_data)} ‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå")
        
        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset
        dataset = await self.create_dataset_from_links(links_data, max_pages)
        
        return dataset

async def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
    print("ü§ñ === Crawl4AI Dataset Scraper ===\n")
    
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    scraper = Crawl4AiDatasetScraper()
    
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á URLs ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ
    test_urls = [
        "https://ratchakitcha.soc.go.th",
        "https://news.thaigov.go.th", 
        "https://www.bangkokpost.com",
        "https://www.thairath.co.th",
    ]
    
    try:
        print("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:")
        print("1. ‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥")
        print("2. ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß")
        print("3. ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å URL ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß")
        
        choice = input("\n‡∏õ‡πâ‡∏≠‡∏ô‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç (1-3): ").strip()
        
        if choice == "1":
            # ‡πÇ‡∏´‡∏°‡∏î‡∏™‡πÅ‡∏Å‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
            url = input(f"‡∏õ‡πâ‡∏≠‡∏ô URL ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏î Enter ‡πÉ‡∏ä‡πâ {test_urls[0]}): ").strip()
            if not url:
                url = test_urls[0]
            
            max_pages = input("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô 20): ").strip()
            max_pages = int(max_pages) if max_pages.isdigit() else 20
            
            keywords_input = input("‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ, ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏á): ").strip()
            keywords = [k.strip() for k in keywords_input.split(',') if k.strip()] if keywords_input else None
            
            dataset = await scraper.auto_scrape_website(url, max_pages, keywords)
            
            if dataset:
                await scraper.save_dataset_to_csv(dataset, "auto_scraped_dataset.csv")
                await scraper.save_dataset_to_json(dataset, "auto_scraped_dataset.json")
                
                print(f"\nüìä ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ:")
                for i, item in enumerate(dataset[:3], 1):
                    print(f"\n{i}. ID: {item['id']}")
                    print(f"   URL: {item['url']}")
                    print(f"   Title: {item['title'][:100]}...")
                    print(f"   Text: {item['text'][:200]}...")
        
        elif choice == "2":
            # ‡πÇ‡∏´‡∏°‡∏î‡∏î‡∏∂‡∏á‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå
            url = input(f"‡∏õ‡πâ‡∏≠‡∏ô URL (‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏î Enter ‡πÉ‡∏ä‡πâ {test_urls[0]}): ").strip()
            if not url:
                url = test_urls[0]
            
            links = await scraper.extract_links_from_page(url)
            
            print(f"\nüîó ‡∏û‡∏ö‡∏•‡∏¥‡πâ‡∏á‡∏Ñ‡πå {len(links)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£:")
            for i, link in enumerate(links[:10], 1):
                print(f"{i}. {link['title'][:80]}")
                print(f"   {link['url']}")
        
        elif choice == "3":
            # ‡πÇ‡∏´‡∏°‡∏î‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å URL ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            url = input("‡∏õ‡πâ‡∏≠‡∏ô URL: ").strip()
            if not url:
                print("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏õ‡πâ‡∏≠‡∏ô URL")
                return
            
            content = await scraper.scrape_content_from_url(url)
            if content:
                print(f"\nüìÑ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ:")
                print(f"Title: {content['title']}")
                print(f"URL: {content['url']}")
                print(f"Text: {content['text'][:500]}...")
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô dataset
                dataset = [{
                    'id': 1,
                    'url': content['url'],
                    'title': content['title'],
                    'text': content['text']
                }]
                await scraper.save_dataset_to_csv(dataset, "single_page_dataset.csv")
        
        else:
            print("‚ùå ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
    
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
    except Exception as e:
        logger.error(f"‚ùå Error: {str(e)}")

if __name__ == "__main__":
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Crawl4AI ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
    try:
        import crawl4ai
        print(f"‚úÖ Crawl4AI version: {crawl4ai.__version__}")
    except ImportError:
        print("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Crawl4AI ‡∏Å‡πà‡∏≠‡∏ô: pip install crawl4ai")
        print("‚ùå ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô: crawl4ai-setup")
        exit(1)
    
    # ‡∏£‡∏±‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°
    asyncio.run(main())
